<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>科研 - 分类 - Kaze Site</title>
        <link>http://example.org/categories/%E7%A7%91%E7%A0%94/</link>
        <description>科研 - 分类 - Kaze Site</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 12 Oct 2021 15:20:04 &#43;0800</lastBuildDate><atom:link href="http://example.org/categories/%E7%A7%91%E7%A0%94/" rel="self" type="application/rss+xml" /><item>
    <title>Self-Attention概述</title>
    <link>http://example.org/posts/%E7%A7%91%E7%A0%94/self-attention%E6%A6%82%E8%BF%B0/</link>
    <pubDate>Tue, 12 Oct 2021 15:20:04 &#43;0800</pubDate>
    <author>作者</author>
    <guid>http://example.org/posts/%E7%A7%91%E7%A0%94/self-attention%E6%A6%82%E8%BF%B0/</guid>
    <description><![CDATA[Self-Attention 什么是self-attention？ self-attention会考虑所有的input，去输出output，考虑了全局的资讯。输入几个in]]></description>
</item><item>
    <title>Transformer概述</title>
    <link>http://example.org/posts/%E7%A7%91%E7%A0%94/transformer%E6%A6%82%E8%BF%B0/</link>
    <pubDate>Sun, 10 Oct 2021 10:20:04 &#43;0800</pubDate>
    <author>作者</author>
    <guid>http://example.org/posts/%E7%A7%91%E7%A0%94/transformer%E6%A6%82%E8%BF%B0/</guid>
    <description><![CDATA[Seq2seq 编码器—解码器（encoder-decoder）或者seq2seq模型，这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。]]></description>
</item></channel>
</rss>
