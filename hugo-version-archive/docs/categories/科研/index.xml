<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>科研 - 分类 - Kaze Site</title>
        <link>http://example.org/categories/%E7%A7%91%E7%A0%94/</link>
        <description>科研 - 分类 - Kaze Site</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 12 Oct 2021 15:20:04 &#43;0800</lastBuildDate><atom:link href="http://example.org/categories/%E7%A7%91%E7%A0%94/" rel="self" type="application/rss+xml" /><item>
    <title>Self-Attention概述</title>
    <link>http://example.org/posts/%E7%A7%91%E7%A0%94/self-attention%E6%A6%82%E8%BF%B0/</link>
    <pubDate>Tue, 12 Oct 2021 15:20:04 &#43;0800</pubDate>
    <author>作者</author>
    <guid>http://example.org/posts/%E7%A7%91%E7%A0%94/self-attention%E6%A6%82%E8%BF%B0/</guid>
    <description><![CDATA[Self-Attention 什么是self-attention？ self-attention会考虑所有的input，去输出output，考虑了全局的资讯。输入几个in]]></description>
</item><item>
    <title>Transformer概述</title>
    <link>http://example.org/posts/%E7%A7%91%E7%A0%94/transformer%E6%A6%82%E8%BF%B0/</link>
    <pubDate>Sun, 10 Oct 2021 10:20:04 &#43;0800</pubDate>
    <author>作者</author>
    <guid>http://example.org/posts/%E7%A7%91%E7%A0%94/transformer%E6%A6%82%E8%BF%B0/</guid>
    <description><![CDATA[Seq2seq 编码器—解码器（encoder-decoder）或者seq2seq模型，这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。]]></description>
</item><item>
    <title>经典CNN模型对比</title>
    <link>http://example.org/posts/%E7%A7%91%E7%A0%94/%E7%BB%8F%E5%85%B8cnn%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94/</link>
    <pubDate>Fri, 24 Sep 2021 15:20:04 &#43;0800</pubDate>
    <author>作者</author>
    <guid>http://example.org/posts/%E7%A7%91%E7%A0%94/%E7%BB%8F%E5%85%B8cnn%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94/</guid>
    <description><![CDATA[LeNet LeNet( (conv): Sequential( (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (1): Sigmoid() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (4): Sigmoid() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (fc): Sequential( (0): Linear(in_features=256, out_features=120, bias=True) (1): Sigmoid() (2): Linear(in_features=120, out_features=84, bias=True) (3): Sigmoid() (4): Linear(in_features=84, out_features=10, bias=True) ) ) AlexNet AlexNet( (conv): Sequential( (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4)) (1): ReLU() (2): MaxPool2d(kernel_size=3,]]></description>
</item></channel>
</rss>
