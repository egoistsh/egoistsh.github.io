---
title: MySQL实战45讲
date: 2023-03-23 22:24:39
permalink: /pages/4f8fa0/
---

学习路径：先要会用，然后可以发现问题。

mysql参考资料：

- mysql的官方手册，有定位再去查漏补缺
- 《高性能的mysql》



MDL metadata lock：元数据锁



一些建议：

- 道路千万条，实践第一条。手动搭建一套主备复制结构。平时遇到问题，动手复现。在阅读其他技术文章、图书时，如果觉得自己理解了一个知识点，也一定要尝试设计一个例子来验证它。也建议设置对照的反例，达到融会贯通。
- 怎么证明自己是不是真的把原理弄清楚了？说出来，写出来！如果有人请教你某个知识点，一定要跟他将清楚，不要觉得这是浪费时间。因为这样，依一来可以验证自己确实搞懂了这个知识点，二来可以提升自己的技术表达能力。毕竟你终究要面临和3类人讲清楚原理的时候：老板、晋升答辩的评委、新工作的面试官。
- 知识要成体系，才不容易忘记。
- 手册补全面，案例扫盲点。一开始不要看手册，手册应该是在你知识网络构建的差不多时再看。



# 一、基础架构

大体来说，MySQL可以分为Server层和存储引擎层两部分。

![image-20210914121117751](https://tva1.sinaimg.cn/large/008i3skNly1gug15ascfij614e0u0djp02.jpg)

## 连接器

一般的连接命令：

```mysql
mysql -h$ip -P$port -u
```

完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。

- 如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序 结束执行。

- 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面 的权限判断逻辑，都将依赖于此时读到的权限。

一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不 会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。

连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在**show processlist**命令中看到它。

客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制 的，默认值是8小时。

尽量使用长连接。

但是全部使用长连接后，你可能会发现，有些时候MySQL占用内存涨得特别快，这是因为 MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。

怎么解决这个问题呢？你可以考虑以下两种方案。

1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。

2. 如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。



## 查询缓存

但是大多数情况下我会建议你不要使用查询缓存， 因为查询缓存往往弊大于利。

查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此 很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库 来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。 比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

好在MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语 句，可以用SQL_CACHE显式指定，像下面这个语句一样：

```mysql
select SQL_CACHE * from T where ID = 10;
```

需要注意的是，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有 这个功能了。

## 分析器

### 词法分析

根据输入的字符串和空格进行识别，识别出里面的字符串分别是什么，代表什么。

### 语法分析

根据语法规则，判断输入的sql是否满足mysql语法。

## 优化器

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join） 的时候，决定各个表的连接顺序。

## 执行器

首先判断有没对表T的执行权限。有的话就打开表继续执行。

分为有索引的情况和没索引的情况。

你会在数据库的慢查询日志中看到一个rows_examined的字段，表示这个语句执行过程中扫描了 多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。

在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟引擎扫描行数跟 rows_examined并不是完全相同的。 

# 二、日志系统

与查询流程不同，更新流程还涉及两个重要的日志模块。

- redo log（重做日志）

- binlog（归档日志）



而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是WriteAhead Logging，它的关键点就是**先写日志，再写磁盘**，也就是先写粉板，等不忙的时候再写账本。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvcsomu70rj60u015c41602.jpg" alt="image-20211012202158612" style="zoom:50%;" />

MySQL整体来看，其实就有两块：一块是Server层，它主要做的是MySQL功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板redo log是 InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。

最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有 crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统— — 也就是 redo log来实现crash-safe能力。

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个 能力称为crash-safe。



这两种日志有以下三点不同。 

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。
2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 
3. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。



redo log的写入拆成了两个步骤：prepare和commit，是"两阶段提交"。



当你需要扩容的时候，也就是需要再 多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用**全量备份加上应用binlog**来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。

简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保 持逻辑上的一致。



**思考一个问题：**定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

一天一备，那么如果需要恢复数据的话，只要保证当天的binlog完整即可；一周一备的话就要保证一周的binlog完整；同时频繁全量备份需要更多存储空间，如何选择取决于业务的重要性，对应的指标是**RTO**(目标恢复时间)。

# 三、事务隔离

提到事务，我们会想到ACID

- Atomicity 原子性
- Consistency 一致性
- Isolation 隔离性
- Durability 持久性

这里主要谈隔离性。

当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non- repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。

隔离得越严实，效率就会越低。



SQL标准的事务隔离级别包括：

- 读未提交（read uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。
- 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化（serializable ）：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。



在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。**在“可重复读”隔离级别下，这个视图是在事务启动时创建的**，整个事务存在期间都用这个视图。**在“读提交”隔离级 别下，这个视图是在每个SQL语句开始执行的时候创建的。**这里需要注意的是，“读未提交”隔离 级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避 免并行访问。



Oracle数据库的默认隔离级别其 实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致， 你一定要记得将MySQL的隔离级别设置为“读提交”。mysql默认隔离级别是可重复读。

配置的方式是，将启动参数transaction-isolation的值设置成READ-COMMITTED。你可以用 show variables来查看当前的值。



尽量避免长事务。

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

在MySQL 5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有20GB，而回滚段有200GB的库。最终只好为了清理回滚段，重建整个库。

除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。



MySQL的事务启动方式有以下几种：

1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。
2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。

建议你总是使用set autocommit=1, 通过显式语句的方式来启动事务。



# 四&五、索引

索引场景模型：

- 哈希表：适用于只有等值查询的场景。
- 有序数组：在等值查询和范围查询场景中性能优秀。只适用于静态存储引擎，更新数据代价太高。
- 搜索树：N叉树，读写性能佳，适合磁盘的访问模式，广泛应用在数据库引擎中。
- 跳表、LSM树。。。



在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索 引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。



## InnoDB的索引模型

InnoDB使用B+树索引模型。

每一个索引在InnoDB里对应一棵B+树。



根据叶子节点的内容，索引类型分为**主键索引**和**非主键索引**。

主键索引的叶子节点存储的是整行数据。在InnoDB中，主键索引也被称为聚簇索引（clustered index）。

非主键索引的叶子节点存储的是主键的值。在InnoDB中，非主键索引也被称为二级索引（secondary index）。

因此，基于主键索引和普通索引的查询的区别是，主键查询只需要搜索ID这棵B+树，普通索引查询，要先搜索普通索引树，得到ID后，再搜索ID索引树。这个过程称为**回表**。

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。



主键长度越小， 普通索引的叶子节点就越小， 普通索引占用的空间也就越小。

从性能和存储空间方面考量，**自增主键**往往是更合理的选择。自增主键也不会造成分裂和合并操作。



Q：通过两个alter 语句重建索引k，以及通过两个alter语句重建主键索引是否合理？

重建索引k的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。不论是删除主键还是创建主键，都会将整个表重建。



## 覆盖索引

由于**覆盖索引**可以减少树的搜索次数， 显著提升查询性能， 所以使用覆盖索引是一个常用的性能优化手段。



我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求， 我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？

如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它 可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。

当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑 了。



## 最左前缀原则

在建立联合索引的时候， 如何安排索引内的字段顺序？

这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。因此，第一原则是， 如果通过调整顺序， 可以少维护一个索引， 那么这个顺序往往就是需要优先考虑采用的。

如果不得不维护另一个索引，那么要考虑空间。



## 索引下推

在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。 

而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

# 六、全局锁和表锁

## 全局锁

根据加锁的范围， MySQL里面的锁大致可以分成**全局锁、表级锁和行锁**三类 。

顾名思义，全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括 建表、修改表结构等）和更新类事务的提交语句。

全局锁的典型使用场景是， 做**全库逻辑备份**。 也就是把整库每个表都select出来存成文本。



不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。

说到视图你肯定想起来了，我们在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视 图的，对吧？

是的，就是在可重复读隔离级别下开启一个事务。

官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导 数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。

你一定在疑惑，有了这个功能，为什么还需要FTWRL呢？一致性读是好，一致性读是好， 但前提是引擎要支 持这个隔离级别。 比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是 只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。

所以，single-transaction方法只适用于所有的表使用事务引擎的库。 如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。

## 表级锁

MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

表锁的语法是 lock tables … read/write。 与FTWRL类似，可以用unlock tables主动释放锁， 也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。



另一类表级的锁是 MDL（ metadata lock)。 MDL不需要显式使用，在访问一个表的时候会被 自动加上。MDL的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

因此，在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当 要对表做结构变更操作的时候，加MDL写锁。



Q：如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频 繁，而你不得不加个字段，你该怎么做呢？

比较理想的机制是，在alter table语句里面 设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后 面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。

MariaDB已经合并了AliSQL的这个功能，所以这两个开源分支目前都支持DDL NOWAIT/WAIT n这个语法。



## 小结

- 全局锁主要用在逻辑备份过程中。对于全部是InnoDB引擎的库，我建议你选择使用–singletransaction参数，对应用会更友好。
- **表锁一般是在数据库引擎不支持行锁的时候才会被用到的**。如果你发现你的应用程序里有lock tables这样的语句，你需要追查一下，比较可能的情况是：
  - 要么是你的系统现在还在用MyISAM这类不支持事务的引擎，那要安排升级换引擎； 
  - 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。
  - MDL会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

# 九、普通索引和唯一索引的选择

由于唯一索引用不上change buffer的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑非唯一索引。

# 十、MySQL为什么有时候会选错索引？

由于索引统计的更新机制，统计的行数有时会有偏差。

而且优化器存在选错索引的可能性。优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的CPU资源越少。

扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

对于由于索引统计信息不准确导致的问题，你可以用analyze table t 语句来解决。（重新统计索引信息）

而对于其他优化器误判的情况，你可以在应用端用force index来强行指定索引，也可以通过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。



估计记录数

mysql通过统计信息来估算记录数，这个统计信息就是索引的“区分度”。一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同值的个数，称为“基数”（cardinality）。

可使用show index from t，查看索引的基数。

mysql通过采样统计来得到索引的基数。InnoDB默认选择N个数据页，统计这些页面上的不通值，得到一个平均值，然后乘以这索引的页面数，得到这个索引的基数。当数据行数超过1/M的时候，会自动触发重新做一次统计所有。

在MySQL中，有两种存储索引统计的方式，可以通过设置参数innodb_stats_persistent的值来选择：

- 设置为on的时候，表示统计信息会持久化存储。这时，默认的N是20，M是10。

- 设置为off的时候，表示统计信息只存储在内存中。这时，默认的N是8，M是16。 
- 由于是采样统计，所以不管N是20还是8，这个基数都是很容易不准的。

# 十一、怎么给字符串字段加索引？

mysql支持前缀索引，可以定义字符串的一部分作为索引。默认索引包含整个字符串。

```sql
alter table user add index index1(email);

alter table user add index index2(email(6));
```

使用前缀索引可以减少空间占用，这是优势。但**可能导致查询语句读数据的次数变多**。

因此使用前缀索引，需要定义好长度，达到即省空间，又不用额外增加太多的查询成本。



可以使用一下方法判断前缀的合适长度：(设定一个可以接受的损失比例)

```sql
select count(distinct email) as L from user;

select
	count(distinct left(emial, 4)) as L4,
	count(distinct left(emial, 5)) as L5,
	count(distinct left(emial, 6)) as L6,
	count(distinct left(emial, 7)) as L7,
from user;
```

此外，使用前缀索引就用不上覆盖索引对查询性能的优化。



前缀多数相同时的索引思路：

- 倒序存储
- 增加hash字段，如用crc()得到校验码。



总结：

1. 直接创建完整索引，这样可能比较占用空间；
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。

# 十二、为什么我的SQL会“抖”一下

当内存数据页跟磁盘数据页内容不一致的时候， 我们称这个内存页为 “ 脏页 ” 。

平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）。



什么情况会引发数据库flush？

- InnoDB的redo log写满了。这时候系统会停止所有更新操作，把 checkpoint往前推进，redo log留出空间可以继续写。
- 系统内存不足。需要淘汰一些数据页，如果淘汰的是脏页就要先把脏页写到磁盘。
- mysql认为系统空闲时。
- mysql正常关闭时。



所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：

1. 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；

2. 日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。 

所以，InnoDB需要有控制脏页比例的机制，来尽量避免上面的这两种情况。



通过innodb_io_capacity告诉InnoDB你的磁盘能力，全力刷脏页时可以多快。建议设为磁盘的IOPS。

参数innodb_max_dirty_pages_pct是脏页比例上限，默认值是75%。

平时要多关注脏页比例， 不要让它经常接近 75%。

innodb_flush_neighbors 参数值为1的时候会有“连坐”机制，值为0时表示不找邻居，自己刷自己的。在MySQL 8.0中，innodb_flush_neighbors参数的默认值已经是0了。



# 十三、为什么表数据删掉一半，表文件大小不变？

如果要收缩一个表，只是delete掉表里面不用的数据的话，表文件的大小是不会变的，你还要通过alter table命令重建表，才能达到表文件变小的目的。我跟你介绍了重建 表的两种实现方式，Online DDL的方式是可以考虑在业务低峰期使用的，而MySQL 5.5及之前的 版本，这个命令是会阻塞DML的，这个你需要特别小心。



optimize table、analyze table和alter table这三种方式重建表的区别。

- 从MySQL 5.6版本开始，alter table t engine = InnoDB（也就是recreate）默认的就是上面图4的流程了；
- analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了MDL读锁；
- optimize table t 等于recreate+analyze。

# 十四、count(*)这么慢，怎么办？

- MyISAM表虽然count(*)很快，但是不支持事务； 
- show table status命令虽然返回很快，但是不准确；
- InnoDB表直接count(*)会遍历全表，虽然结果准确，但会导致性能问题。



用redis或者是用另一张表记录总数，都会出现数据不一致的情况。

把计数放在Redis里面，不能够保证计数和MySQL表里的数据精确一致的原因，是这两个这两个不同的存储构成的系统， 不支持分布式事务， 无法拿到精确一致的视图。 

而把计数值也放在 MySQL中，就解决了一致性视图的问题。 InnoDB引擎支持事务，我们利用好事务的原子性和隔离性，就可以简化在业务开发时的逻辑。 这也是InnoDB引擎备受青睐的原因之一。

问题都是由于InnoDB要支持事务，从而导致InnoDB表不能把count(*) 直接存起来，然后查询的时候直接返回形成的。 所谓以子之矛攻子之盾，现在我们就利用“事务”这个特性，把问题解决掉。

![image-20211010225722719](https://tva1.sinaimg.cn/large/008i3skNly1gvalxjks5pj61i60u0aca02.jpg)



count()的语义。count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。

所以，count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数。

分析性能差别：

- server层要什么就给什么；
- InnoDB只给必要的值；
- 现在的优化器只优化了count(*)的语义为“取行数”，其他“显而易见”的优化并没有做。

结论是：按照效率排序的话，count(字段)<count(主键id)<count(1)≈count(*)，所以我建议你，尽量使用count(\*)。



# 十五、答疑，日志和索引相关

崩溃恢复时的判断规则：

1. 如果redo log里面的事务是完整的，也就是已经有了commit标识，则直接提交。
2. 如果redo log里面的事务只有完整的prepare，则判断对应的事务binlog是否存在并完整：
   1. 如果是，则提交事务。
   2. 否则，回滚事务。



mysql怎么知道binlog是否完整？

一个事务的binlog是由完整格式的：

- statement格式的binlog，最后会有COMMIT；
- row格式的binlog，最后会有一个XID event；

另外，在MySQL 5.6.2后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。



redo log 和 binlog是怎么关联起来的？

他们有一个共同的数据字段，叫XID，崩溃恢复时，会按顺序扫描redo log：

- 如果碰到既有prepare，又有commit的redo log，直接提交。
- 如果碰到只有prepare，没有commit的redo log，就拿着XID去binlog找对应的事务。



# 十六、order by是怎么工作的？

通常情况下，全字段排序：

1. 初始化sort_buffer，确定放入name、city、age这三个字段；

2. 从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X；

3. 到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中；

4. 从索引city取下一个记录的主键id；

5. 重复步骤3、4直到city的值不满足查询条件为止，对应的主键id也就是图中的ID_Y；

6. 对sort_buffer中的数据按照字段name做快速排序；

7. 按照排序结果取前1000行返回给客户端。





如果mysql认为排序的单行长度太大会采用rowid排序：

（即新的算法放入sort_buffer的字段，只有要排序的列（即name字段）和主键id，然后再从原表取）

1. 初始化sort_buffer，确定放入两个字段，即name和id；

2. 从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X；

3. 到主键id索引取出整行，取name、id这两个字段，存入sort_buffer中；

4. 从索引city取下一个记录的主键id；

5. 重复步骤3、4直到不满足city='杭州’条件为止，也就是图中的ID_Y；

6. 对sort_buffer中的数据按照字段name进行排序；

7. 遍历排序结果，取前1000行，并按照id的值回到原表中取出city、name和age三个字段返回

给客户端。



如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过 程中一次可以排序更多行，但是需要再回到原表去取数据。 

如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。 

这也就体现了MySQL的一个设计思想：如果内存够，如果内存够， 就要多利用内存， 尽量减少磁盘访问。



优化：（索引的维护有代价，需要权衡）

- 如果从索引上取出来的行，天然就排序好了的话，就可以不用再排序了。可以使用联合索引。
- 还可以覆盖索引，使其直接返回需要的值。（也是用上联合索引）



# 十七、如何正确地显示随机消息

```sql
# 随机选择3个单词
select word from words order by rand() limit 3;
```

这条语句的执行流程是这样的：

1. 创建一个临时表。这个临时表使用的是memory引擎，表里有两个字段，第一个字段是 double类型，为了后面描述方便，记为字段R，第二个字段是varchar(64)类型，记为字段

W。并且，这个表没有建索引。

2. 从words表中，按主键顺序取出所有的word值。对于每一个word值，调用rand()函数生成一

个大于0小于1的随机小数，并把这个随机小数和word分别存入临时表的R和W字段中，到 此，扫描行数是10000。 3. 现在临时表有10000行数据了，接下来你要在这个没有索引的内存临时表上，按照字段R排

序。 4. 初始化 sort_buffer。sort_buffer中有两个字段，一个是double类型，另一个是整型。

5. 从内存临时表中一行一行地取出R值和位置信息（我后面会和你解释这里为什么是“位置信 息”），分别存入sort_buffer中的两个字段里。这个过程要对内存临时表做全表扫描，此时 扫描行数增加10000，变成了20000。

6. 在sort_buffer中根据R的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫

描行数。 

7. 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出word值，返回给客户

端。这个过程中，访问了表的三行数据，总扫描行数变成了20003。



小结：order by rand()使用了内存临时表， 内存临时表排序的时候使用了 rowid排序方法。

- 对于对于 InnoDB表来说 ，执行全字段排序会减少磁盘访问，因此会被优先选择。

- 而对于内存表， 回表过程只是简单地根据数据行的位 置， 直接访问内存得到数据， 根本不会导致多访问磁盘 。优化器没有了这一层顾虑，那么它 会优先考虑的，就是用于排序的行长度越少越好了，所以，MySQL这时就会选择rowid排序。



tmp_table_size配置内存临时表的大小，默认是16M，超过时会转成磁盘临时表。

MySQL 5.6版本引入的一个新的排序算法， 即：优先队列排序算法。（维护了一个堆）

临时文件的算法也就是归并算法。

如果你直接使用order by rand()，这个语句需要Using temporary 和 Using filesort，查询的执行代价往往是比较大的。所以，在设计的时候你要量避开这种写法。



为了达到严格的随机结果：

1. 取到整个表的行数，记作C。
2. 取到Y = floor(C * rand())。floor在这里的作用，就是取整数部分。
3. 再用limit Y, 1取得一行。

# 十八、为什么逻辑相同的SQL语句，性能差异巨大（先pass）

小结：对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。

# 十九、为什么只查一行的语句，也执行慢？

分析原因时候，一般先执行`show processlist`，看看当前语句处于什么状态。



第一类：查询长时间不返回

- 等MDL锁

  出现Waiting for table metadata lock，表示现在有一个线程正在表t上请求或者持有MDL写锁，把select语句堵住了。

  解决：通过查询`select blocking_pid from sys.schema_table_lock_waits;`查出造成阻塞的process id，把他kill了。

- 等flush

- 等行锁

第二类：查询慢

# 二十、幻读是什么？有什么问题？（还是有点蒙呀）

for update 排他锁

for update仅适用于InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。在进行事务操作时，通过“for update”语句，MySQL会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞。排他锁包含行锁、表锁。

lock in share mode 共享锁



幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

“幻读”做一个说明：

1. 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此， 幻读在“当前读”下才会出现。

2. 上面session B的修改结果，被session A之后的select语句用“当前读”看到，不能称为幻读。 幻读仅专指“新插入的行”。



锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此 刻的一致性，还包含了数据和日志在逻辑上的一致性。



InnoDB如何解决幻读？

幻读产生的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB只好引入新的锁，也即是间隙锁（Gap Lock）。

间隙锁和行锁合称next-key lock。但其实他们是分开加的。



间隙锁是在可重复读隔离级别下才会生效的。所以，你如果把隔离级别设置为读提交的话， 就没有间隙锁了。但同时，你要解决可能出现的数据和日志不一致问题，需要把binlog格式设置 为row。这也是现在不少公司使用的配置组合。



很多公司使用：读提交隔离级别加binlog_format=row的组合。到底合不合理？

如果读提交隔离级别够用，即业务不需要可重复读的保证，这样考虑到读提交下操作数据的锁范围更小（没有间隙锁），这个选择是合理的。



比如大家都用读提交，可逻辑备份时，mysqldump为什么要把备份线程设置成可重复读？然后，在备份期间，备份线程用的是可重复读，而业务线程使用的是读提交，同时存在两种事务隔离级别会不会有问题？

# 二十一、加锁规则

## 可重复读隔离级别下

锁是加在索引上的。

next-key lock=间隙锁+行锁，(]，前开后闭区间。

间隙锁，()，前开后开区间。

行锁就一个。



加锁规则：2个原则，2个优化，1个bug

1. 原则1：加锁的基本单位是next-key lock。
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且遇到不满足等值条件的值时，next-key lock退化为间隙锁。

5. 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。



## 读提交隔离级别下

读提交隔离级别的话，去掉间隙锁的部分，剩下行锁的部分。

其实读提交隔离级别在外键场景下还是有间隙锁，相对比较复杂，我们今天先不展开。

另外，在读提交隔离级别下还有一个优化，即：语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交。

也就是说，读提交隔离级别下，锁的范围更小，锁的时间更短，这也是不少业务都默认使用读提交隔离级别的原因。



# 24、mysql怎么保持主备一致？

binlog功不可没。

mysql有容易学习和方便使用高可用架构。几乎所有的高可用架构，都直接依赖于binlog。虽然越来与复杂，但都是从最基本的一主一备演化而来。



在一个主备关系中，每个备库接受主库的binlog执行。只要主库执行更新生成的所有binlog都可以传到备库被正确执行，备库就能达到跟主库一致的状态，这就是**最终一致性**。



建议把备库设置成readonly模式。好处如下：

- 有些运营类的查询语句会再备库查，设置为只读可防止误操作。
- 防止切换逻辑bug，如切换过程中出现双写，造成主备不一致。
- 可用readonly状态，判断节点的角色。

因为readonly设置对super权限角色无效，而用于同步更新的线程就拥有超级权限。



主库和备库之间维持了一个长连接。主库内部有一个线程，专门用于服务备库的这个长连接。



binlog的三种格式：（通过binlog_format属性设置）

- statement。记录的是语句原文。可能导致主备不一致。
- row。记录了真实删除行的主键id（如果是删除）。占空间，如删除10w条数据。
- mixed。前两种的混合。

现在越来越多的场景要求把mysql的binlog格式设置为row，其中之一好处在于：恢复数据。



实际生产使用比较多的是双M结构，互为主备。相较于MS结构，切换时不用修改主备关系。

mysql通过判断server id的方式，解决循环复制问题。



通过mysqlbinlog工具解析和查看binlog中的内容。

# 25、mysql如何保证高可用？

只有最终一致性还不够，mysql还要提供高可用。



## 主备延迟

可通过在备库执行 `show slave status`显示`seconds_behind_master`表示当前备库延迟了多少秒。



主备延迟的来源：

- 备库所在的机器性能比主库的差。（现在一般做对称部署，主备切换）。
- 备库的压力大。
- 主库大事务。
  - 因为主库必须等事务执行完成才会写入binlog，再传给备库。
  - 如不要一次性用delete删除太多数据，典型的大事务场景。
  - 如大表DDL也是典型的大事务场景。处理方案：计划内的DDL，建议使用go-ost方案。
- 备库的并行复制能力。



一般处理：

- 一主多从。（这里把HA过程中被选成新主库的称为备库，其他的称为从库。备库和从库概念上差不多）
- 通过binlog输出到外部系统，如hadoop，让外部系统提供统计类查询的能力。



### 可靠性优先策略：

双M结构下，主备切换： 

1. 判断备库B现在的seconds_behind_master，如果小于某个值（比如5秒）继续下一步，否则

持续重试这一步；

2. 把主库A改成只读状态，即把readonly设置为true；

3. 判断备库B的seconds_behind_master的值，直到这个值变成0为止；

4. 把备库B改成可读写状态，也就是把readonly 设置为false；

5. 把业务请求切到备库B。

这个切换流程，一般是由专门的HA系统来完成的，我们暂时称之为**可靠性优先流程**。



### 可用性有先策略：

如果我强行把步骤4、5调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库 B，并且让备库B可以读写，那么系统几乎就没有不可用时间了。

这个切换流程的代价，就是可能出现数据不一 致的情况。



结论：

1. 使用row格式的binlog时，数据不一致的问题更容易被发现。而使用mixed或者statement格式的binlog时，数据很可能悄悄地就不一致了。如果你过了很久才发现数据不一致的问题， 很可能这时的数据不一致已经不可查，或者连带造成了更多的数据逻辑不一致。

2. 主备切换的可用性优先策略会导致数据不一致。因此，大多数情况下，我都建议你使用可靠性优先策略。毕竟对数据服务来说的话，数据的可靠性一般还是要优于可用性的。



在满足数据可靠性的前提下，MySQL高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。

# 26、备库为什么会延迟好几个小时？（略看）

简单总结：

介绍了MySQL的各种**多线程复制策略**。

为什么要有多线程复制呢？这是因为单线程复制的能力全面低于多线程复制，对于更新压力较大 的主库，备库是可能一直追不上主库的。从现象上看就是，备库上seconds_behind_master的值 越来越大。

大事务不仅会影响到主库，也是造成备库复制延迟的主要原因之一。 因此，在平时的开发工作中，我建议你尽量减少大事务操作，把大事务拆成小事务。



binlog-transaction-dependency-tracking参数：

- COMMIT_ORDER
- WRITESET
- WRITE_SESSIOn

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvm2exp1kjj617h0u0q4p02.jpg" alt="image-20211020204851918" style="zoom:50%;" />

# 27、主库出问题了从库怎么办？

![image-20211020205044594](https://tva1.sinaimg.cn/large/008i3skNly1gvm2gtrqt3j61850u0mzs02.jpg)

一主多从基本结构。



将节点B设置成节点A的从库A`，需要执行change master命令。

这条命令有6个参数，其中MASTER_LOG_FILE和MASTER_LOG_POS表示从主库的【文件名】、【日志偏移量】的日志继续同步。

这个pos往往不精确。

通常我们在切换任务时，要先主动跳过这些错误，有两种常用的方法：

- 主动跳过一个事务`set global sql_slave_skip_count = 1;start save;`
- 通过设置`slave_skip_error`参数，直接设置跳过指定的错误。
  - 1062错误是插入数据时唯一键冲突；
  - 1032错误是删除数据时找不到行。

## GTID

通过sql_slave_skip_counter跳过事务和通过slave_skip_errors忽略错误的方法，虽然都最终可以 建立从库B和新主库A’的主备关系，但这两种操作都很复杂，而且容易出错。所以，MySQL 5.6版本引入了GTID，彻底解决了这个困难。



GTID的全称是Global Transaction Identifier，也就是全局事务ID，是一个事务在提交的时候生成的，是这个事务的唯一标识。

`GTID=server_uuid:gno`

- server_uuid是一个实例第一次启动时自动生成的，是一个全局唯一的值； 

- gno是一个整数，初始值是1，每次提交事务的时候分配给这个事务，并加1。



GTID主备复制的用法：

执行change maste，只需使用`master_auto_position = 1`代替前面的MASTER_LOG_FILE和MASTER_LOG_POS。



在基于GTID的主备关系里，系统认为只要建立主备关系，就必须保证主库发给备库的日志是完整的。因此，如果实例B需要的日志已经不存在，A’就拒绝把日志发给B。

这跟基于位点的主备协议不同。基于位点的协议，是由备库决定的，备库指定哪个位点，主库就发哪个位点，不做日志的完整性判断。



# 28、读写分离

一主多从架构的应用场景：读写分离。

读写分离的主要目标就是分摊主库的压力。

![image-20211021224738188](https://tva1.sinaimg.cn/large/008i3skNly1gvnbh6i76vj314a0mgabo.jpg)

客户端主动做读写分离。

![image-20211021225015121](https://tva1.sinaimg.cn/large/008i3skNly1gvnbjhiczrj61g80r0dhf02.jpg)

中间代理层proxy分发。



方案：

- 强制走主库方案
- sleep方案
- 判断主备无延迟方案
  - 判断seconds_behind_master 是否等于0
  - 对比位点qu确保主备无延迟
  - 对比GTID集合确保主备无延迟
- 配合semi-sync方案
- 等主库位点方案
- 等GTID方案



## 判断主备无延迟方案

## semi-sync replication 半同步控制

1. 事务提交的时候，主库把binlog发给从库；

2. 从库收到binlog以后，发回给主库一个ack，表示收到了；

3. 主库收到这个ack以后，才能给客户端返回“事务完成”的确认。



semi-sync配合前面关于位点的判断，就能够确定在从库上执行的查询请求，可以避免过期读。

但是，semi-sync+位点判断的方案，只对一主一备的场景是成立的。在一主多从场景中，主库只 要等到一个从库的ack，就开始给客户端返回确认。



小结一下，semi-sync配合判断主备无延迟的方案，存在两个问题：

1. 一主多从的时候，在某些从库执行查询请求会存在过期读的现象；

2. 在持续延迟的情况下，可能出现过度等待的问题。

等主库位点方案，就可以解决这两个问题



## 等主库位点方案

`select master_pos_wait(file, pos[, timeout]);`

1. 它是在从库执行的；

2. 参数file和pos指的是主库上的文件名和位置；

3. timeout可选，设置为正整数N表示这个函数最多等待N秒。

这个命令正常返回的结果是一个正整数M，表示从命令开始执行，到应用完file和pos表示的 binlog位置，执行了多少事务。

当然，除了正常返回一个正整数M外，这条命令还会返回一些其他结果，包括：

1. 如果执行期间，备库同步线程发生异常，则返回NULL；
2. 如果等待超过N秒，就返回-1；
3. 如果刚开始执行的时候，就发现已经执行过这个位置了，则返回0。



1. trx1事务更新完成后，马上执行show master status得到当前主库执行到的File和Position；

2. 选定一个从库执行查询语句；

3. 在从库上执行select master_pos_wait(File, Position, 1)；

4. 如果返回值是>=0的正整数，则在这个从库执行查询语句；

5. 否则，到主库执行查询语句。



如果按照我们设定不允许过期读的要求，就只有两种选择，一种是超时放弃，一种是转到主库查询。具体怎么选择，就需要业务开发同学做好限流策略了。



## GTID方案

如果你的数据库开启了GTID模式，对应的也有等待GTID的方案。

`select wait_for_executed_gtid_set(gtid_set, 1);`

这条命令的逻辑是：

1. 等待，直到这个库执行的事务中包含传入的gtid_set，返回0；

2. 超时返回1。



在前面等位点的方案中，我们执行完事务后，还要主动去主库执行show master status。而 MySQL 5.7.6版本开始，允许在执行完更新类事务后，把这个事务的GTID返回给客户端，这样等GTID的方案就可以减少一次查询。 这时，等GTID的执行流程就变成了：

1. trx1事务更新完成后，从返回包直接获取这个事务的GTID，记为gtid1；

2. 选定一个从库执行查询语句；

3. 在从库上执行 select wait_for_executed_gtid_set(gtid1, 1)；

4. 如果返回值是0，则在这个从库执行查询语句；

5. 否则，到主库执行查询语句。

跟等主库位点的方案一样，等待超时后是否直接到主库查询，需要业务开发同学来做限流考虑。



在实际应用中，这几个方案是可以混合使用的。

比如，先在客户端对请求做分类，区分哪些请求可以接受过期读，而哪些请求完全不能接受过期读；然后，对于不能接受过期读的语句，再使用等GTID或等位点的方案。

但话说回来，过期读在本质上是由一写多读导致的。在实际应用中，可能会有别的不需要等待就可以水平扩展的数据库方案，但这往往是用牺牲写性能换来的，也就是需要在读性能和写性能中取权衡。



开源proxy：MariaDB MaxScale

# 40、InnoDB和Memory引擎

InnoDB表的数据就放在主键索引树上，主键索引是B+树。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqhyxsi7ej60x40fgt9702.jpg" alt="image-20211024164923563" style="zoom: 67%;" />

Memory引擎的数据和索引是分开的。

内存表的数据部分以数组的形式单独存放，而主键id索引里，存放的是每个数据的位置。**主键id是hash索引**，索引上的key不是有序的。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqhzjei4tj614e0mwgmq02.jpg" alt="image-20211024164956388" style="zoom:67%;" />

可见，InnoDB和Memory引擎的数据组织方式是不同的：

- InnoDB引擎把数据放在主键索引上，其他索引上保存的是主键id。这种方式，我们称之为**索引组织表** （Index Organizied Table）。

- 而Memory引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之 为**堆组织表** （Heap Organizied Table）。



这两个引擎的一些典型不同：

1. InnoDB表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的；
2. 当数据文件有空洞的时候，InnoDB表在插入新数据的时候，为了保证数据有序性，只能在固定的位置写入新值，而内存表找到空位就可以插入新值； 
3. 数据位置发生变化的时候，InnoDB表只需要修改主键索引，而内存表需要修改所有索引；
4. InnoDB表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找。而内存表没有这个区别，所有索引的“地位”都是相同的。
5. InnoDB支持变长数据类型，不同记录的长度可能不同；内存表不支持Blob 和 Text字段，并且即使定义了varchar(N)，实际也当作char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同。



## hash索引和b-tree索引

实际上，内存表也是自持B-Tree索引的。

在我们的一般印象中，内存表的优势是速度快，其中一个原因就是Memory引擎支持hash索引。更重要的原因是，内存表的所有数据都保存在内存上，而内存的读写速度总是比磁盘快。



但不建议在生产环境上使用内存表，主要有2个原因：

1. 锁粒度问题。
   1. 内存表不支持行锁，只支持表锁。
2. 数据持久化问题



建议把普通内存表都用InnoDB表代替。

例外的场景：内存临时表。内存表支持hash索引，这个特性利用起来，对复杂查询的加速效果还是很不错的。

# 40、insert语句的锁为什么这么多？（初略总结）

MySQL对自增主键锁做了优化，尽量在申请到自增id以后，就释放自增锁。

普通情况下，insert语句是一个很轻量的操作。



特殊情况下，如：

insert …select 是很常见的在两个表之间拷贝数据的方法。你需要注意，在可重复读隔离级别下，这个语句会给select的表里扫描到的记录和间隙加读锁。 而如果insert和select的对象是同一个表，则有可能会造成循环写入。这种情况下，我们需要引入

用户临时表来做优化。

insert 语句如果出现唯一键冲突，会在冲突的唯一值上加共享的next-key lock(S锁)。因此，碰到

由于唯一键约束导致报错后，要尽快提交或回滚事务，避免加锁时间过长。

# 43、要不要使用分区表？

分区表有什么问题，为什么很多公司规范不让使用分区表呢？



分区表的组织形式：

```sql
CREATE TABLE `t` (
	`ftime` datetime NOTNULL,
  `c` int(11) DEFAULT NULL,
  KEY (`ftime`)
) ENGINE=InnoDB DEFAULT CHARSET = latin1
PARTITION BY RANGE(YEAR(ftime))
(PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB,
 PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB,
 PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB,
 PARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB);
 
 insert into t value('2017-4-1',1),('2018-4-1',1); 
```

在表t中初始化插入了两行记录，按照定义的分区规则，这两行记录分别落在p_2018和p_2019 这两个分区上。



这个表包含了一个.frm文件和4个.ibd文件，每个分区对应一个.ibd文件。也就是说：

- 对于引擎层来说，这是4个表； 
- 对于Server层来说，这是1个表。



我们使用分区表的一个重要原因就是单表过大。那么，如果不使用分区表的话，我们就是要使用手动分表的方式。

分区表和手工分表，一个是由server层来决定使用哪个分区，一个是由应用层代码来决定使用哪 个分表。因此，从引擎层看，这两种方式也是没有差别的。

其实这两个方案的区别，主要是在server层上。从server层看，我们就不得不提到分区表一个被广为诟病的问题：打开表的行为。

1. MySQL在第一次打开分区表的时候，需要访问所有的分区；

2. 在server层，认为这是同一张表，因此所有分区共用同一个MDL锁；

3. 在引擎层，认为这是不同的表，因此MDL锁之后的执行过程，会根据分区表规则，只访问必要的分区。



## 分区表的应用场景

分区表的一个显而易见的优势是对业务透明，相对于用户分表来说，使用分区表的业务代码更简洁。还有，分区表可以很方便的清理历史数据。

如果一项业务跑的时间足够长，往往就会有根据时间删除历史数据的需求。这时候，按照时间分区的分区表，就可以直接通过**alter table t drop partition**这个语法删掉分区，从而删掉过期的历史数据。

这个alter table t drop partition …操作是直接删除分区文件，效果跟drop普通表类似。与使用delete语句删除数据相比，优势是速度快、对系统影响小。



我们以范围分区（range）为例和你介绍的。实际上，MySQL还支持hash分区、 list分区等分区方法。你可以在需要用到的时候，再翻翻[手册](https://dev.mysql.com/doc/refman/8.0/en/partitioning-types.html)。



实际使用时，分区表跟用户分表比起来，有两个绕不开的问题：一个是第一次访问的时候需要访问所有分区，另一个是共用MDL锁。

因此，如果要使用分区表，就不要创建太多的分区。我见过一个用户做了按天分区策略，然后预先创建了10年的分区。这种情况下，访问分区表的性能自然是不好的。这里有两个问题需要注意：

1. 分区并不是越细越好。实际上，单表或者单分区的数据**一千万行**，只要没有特别大的索引， 对于现在的硬件能力来说都已经是小表了。

2. **分区也不要提前预留太多**，在使用之前预先创建即可。比如，如果是按月分区，每年年底时再把下一年度的12个新分区创建上即可。对于没有数据的历史分区，要**及时的drop掉**。

至于分区表的其他问题，比如查询需要跨多个分区取数据，查询性能就会比较慢，基本上就不是分区表本身的问题，而是数据量的问题或者说是使用方式的问题了。

当然，如果你的团队已经维护了成熟的分库分表中间件，用业务分表，对业务开发同学没有额外的复杂性，对DBA也更直观，自然是更好的。

